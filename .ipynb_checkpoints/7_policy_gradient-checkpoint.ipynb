{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4fef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baff53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9894bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(keras.Model):\n",
    "    def __init__(self, action_dim=1):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = layers.Dense(24, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(36, activation=\"relu\")\n",
    "        self.fc3 = layers.Dense(action_dim, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def process(self, observations):\n",
    "        # Process batch observations using `call(x)` behind-the-scenes\n",
    "        action_probabilities = self.predict_on_batch(observations)\n",
    "        return action_probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05499e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, action_dim=1):\n",
    "        \"\"\"Agent with a neural-network policy\n",
    "        \n",
    "        Args: \n",
    "            action_dim (int): Action dimension\n",
    "        \"\"\"\n",
    "        self.policy_net = PolicyNet(action_dim=action_dim)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "    def policy(self, observation):\n",
    "        observation = observation.reshape(1, -1)\n",
    "        observation = tf.convert_to_tensor(observation, dtype=tf.float32)\n",
    "        action_logits = self.policy_net(observation)\n",
    "        action = tf.random.categorical(tf.math.log(action_logits), num_samples=1)\n",
    "        return action\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        action = self.policy(observation).numpy()\n",
    "        return action.squeeze()\n",
    "    \n",
    "    def learn(self, states, rewards, actions):\n",
    "        discounted_reward = 0\n",
    "        discounted_rewards = []\n",
    "        rewards.reverse()\n",
    "        for r in rewards:\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            discounted_rewards.append(discounted_reward)\n",
    "        discounted_rewards.reverse() \n",
    "        \n",
    "        for state, reward, action in zip(states, discounted_rewards, actions):\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probabilities = self.policy_net(np.array([state]), training=True)\n",
    "                loss = self.loss(action_probabilities, action, reward)\n",
    "            grads = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(grads, self.policy_net.trainable_variables)\n",
    "            )\n",
    "            \n",
    "    def loss(self, action_probabilities, action, reward):\n",
    "        dist = tfp.distributions.Categorical(\n",
    "            probs = action_probabilities, dtype=tf.float32\n",
    "        )\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob * reward\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39b1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, env: gym.Env, episodes: int, render=True):\n",
    "    \"\"\"\n",
    "    Train `agent` in `env` for `episodes`\n",
    "    \n",
    "    Args:\n",
    "        agent(Agent) : Agent to train\n",
    "        env (gym.Env): Environment to train the agent \n",
    "        episodes (int): Number of episodes to train\n",
    "        render (bool) : True=Enable/ False=Disable rendering\n",
    "    \"\"\"\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        done = False \n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = next_state\n",
    "            total_reward += reward \n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                agent.learn(states, rewards, actions)\n",
    "                print(\"\\n\")\n",
    "            print(f\"Episodes #{episode} ep_reward: {total_reward}\", end=\"\\r\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a6f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 17:12:04.984192: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-22 17:12:04.986745: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Episodes #0 ep_reward: -199.0\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'reverse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, env, episodes, render)\u001b[0m\n\u001b[1;32m     29\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodes #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ep_reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self, states, rewards, actions)\u001b[0m\n\u001b[1;32m     28\u001b[0m     discounted_reward \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m discounted_reward\n\u001b[1;32m     29\u001b[0m     discounted_rewards\u001b[38;5;241m.\u001b[39mappend(discounted_reward)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mdiscounted_reward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse\u001b[49m() \n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, reward, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(states, discounted_rewards, actions):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'reverse'"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "episodes = 2\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "train(agent, env, episodes, render=False)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d34ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3812jvsc74a57bd0f14269bf3ad7aa2ae115b9ca9481e15e8eacecd83dc3347ac1efd388ad78cc6e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
