{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a980a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.critic = tf.keras.layers.Dense(1, activation='linear')\n",
    "        self.actor = tf.keras.layers.Dense(action_dim, activation='linear')\n",
    "    \n",
    "    def call(self, input_data):\n",
    "        x = self.fc1(input_data)\n",
    "        x1 = self.fc2(x)\n",
    "\n",
    "        actor = self.actor(x1)\n",
    "        critic = self.critic(x1)\n",
    "        return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02895096,  0.01745123,  0.02670889, -0.03680816]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation = observation.reshape([1, -1])\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 12:12:32.022636: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-23 12:12:32.023599: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "actor_critic = ActorCritic(action_dim=action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: `get_action` \n",
    "\n",
    "* Arg: \n",
    "    * state (`np.ndarray`) : observation or state from the environment at current time step\n",
    "\n",
    "\n",
    "* Return:\n",
    "    * action (`int`) : action following the current policy at the time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets action from actor network\n",
    "\n",
    "def get_action(state):\n",
    "    \"\"\"\n",
    "    Gets action following the policy at current time step\n",
    "    Arg: \n",
    "        state (np.ndarray) : observation or state from environment\n",
    "    return: \n",
    "        action (int) : action following the policy \n",
    "    \"\"\"\n",
    "    action = np.random.randint(action_dim)\n",
    "    state = np.array(state).reshape([1, -1])\n",
    "\n",
    "    action_probs, _ = actor_critic(state)\n",
    "    action_probs = tf.nn.softmax(action_probs)\n",
    "    action_probs = action_probs.numpy()\n",
    "\n",
    "    dist = tfp.distributions.Categorical(probs=action_probs, dtype=tf.float32)\n",
    "    action = int(dist.sample())\n",
    "\n",
    "    return action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: `actor_loss`\n",
    "\n",
    "* Args:\n",
    "    * prob (`tf.Tensor`) : raw output from actor network (must be processed by softmax / sigmoid)\n",
    "    * action (`int`) : action from current state  \n",
    "    * td (`tf.Tensor`) : temporal difference target \n",
    "\n",
    "* Return:\n",
    "    * loss (`tf.Tensor`) : loss output which will be used for computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_loss(prob, action, td):\n",
    "    prob = tf.nn.softmax(prob)\n",
    "    dist = tfp.distributions.Categorical(prob, dtype=tf.float32)\n",
    "    return (-1)*(dist.log_prob(action))*td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.00074995, -0.00206248]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.0023877]], dtype=float32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs, value = actor_critic(observation)\n",
    "action_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in one time Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before updating : [[0.00813797]]\n",
      "Prediction after updating : [[-0.19515835]]\n"
     ]
    }
   ],
   "source": [
    "# learning in one step\n",
    "\n",
    "done = False \n",
    "action = get_action(state)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "state, action, reward, next_state \n",
    "\n",
    "state = state.reshape([1, -1])\n",
    "next_state = next_state.reshape([1, -1])\n",
    "\n",
    "print(f\"Prediction before updating : {actor_critic(state)[1]}\")\n",
    "\n",
    "with tf.GradientTape() as tape: \n",
    "    action_probs, value = actor_critic(state, training=True)\n",
    "    _, value_next = actor_critic(next_state, training=True)\n",
    "    td = reward + gamma * value_next * (1-int(done)) - value\n",
    "    loss_a = actor_loss(prob=action_probs, action=action, td=td)\n",
    "    loss_c = td ** 2 \n",
    "    total_loss = loss_a + loss_c\n",
    "grads = tape.gradient(total_loss, actor_critic.trainable_variables)\n",
    "optimizer.apply_gradients(zip(grads, actor_critic.trainable_variables))\n",
    "state = next_state\n",
    "\n",
    "print(f\"Prediction after updating : {actor_critic(state)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learn(\n",
    "    state : np.ndarray,\n",
    "    action,\n",
    "    reward,\n",
    "    next_state: np.ndarray,\n",
    "    done):\n",
    "    \"\"\"Learning function for each step \n",
    "    Args : \n",
    "        state (np.ndarray) : current state (shape = (1, state_dim))\n",
    "        action (int) : current action \n",
    "        reward (float) : reward returned by the environment after taking action        \n",
    "        next_state (np.ndarray) : next state (shape = (1, state_dim))\n",
    "        done (boolean) : whether the episode ends \n",
    "    Return : \n",
    "        loss (tf.Tensor) : returns loss after taking a gradient update step\n",
    "        updated Model (tf.Model) : updated model after taking a gradient update step\n",
    "    \"\"\"\n",
    "    state = state.reshape([1, -1])\n",
    "    next_state = next_state.reshape([1, -1]) \n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs, value = actor_critic(state)\n",
    "        _, next_val = actor_critic(next_state)\n",
    "        td = reward + gamma * next_val * (1-int(done)) - value \n",
    "        loss_a = actor_loss(action_probs, action, td)\n",
    "        loss_c = td ** 2\n",
    "        total_loss = loss_a + loss_c \n",
    "    \n",
    "    grads = tape.gradient(total_loss, actor_critic.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, actor_critic.trainable_variables))\n",
    "    return total_loss\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/tensorflow-test/env/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "done = False \n",
    "action = get_action(state)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "state, action, reward, next_state \n",
    "\n",
    "loss.append(learn(state, action ,reward, next_state, done))\n",
    "state = next_state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [ 0.17739686  0.26038256 -0.24739863 -0.7663703 ] -> next state: [ 0.17739686  0.26038256 -0.24739863 -0.7663703 ]\n",
      "loss = [[0.9072479]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"state {state} -> next state: {next_state}\")\n",
    "print(f\"loss = {loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7782dd8a32840d6a27425c59b85f33d757e9ee643cd74cc2a8dc200067c3786"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
