{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e112a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63267148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb98fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(keras.Model):\n",
    "    def __init__(self, action_dim=1):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = layers.Dense(24, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(36, activation=\"relu\")\n",
    "        self.fc3 = layers.Dense(action_dim, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def process(self, observations):\n",
    "        # Process batch observations using `call(x)` behind-the-scenes\n",
    "        action_probabilities = self.predict_on_batch(observations)\n",
    "        return action_probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56136bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, action_dim=1):\n",
    "        \"\"\"Agent with a neural-network policy\n",
    "        \n",
    "        Args: \n",
    "            action_dim (int): Action dimension\n",
    "        \"\"\"\n",
    "        self.policy_net = PolicyNet(action_dim=action_dim)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "    def policy(self, observation):\n",
    "        observation = observation.reshape(1, -1)\n",
    "        observation = tf.convert_to_tensor(observation, dtype=tf.float32)\n",
    "        action_logits = self.policy_net(observation)\n",
    "        action = tf.random.categorical(tf.math.log(action_logits), num_samples=1)\n",
    "        return action\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        action = self.policy(observation).numpy()\n",
    "        return action.squeeze()\n",
    "    \n",
    "    def learn(self, states, rewards, actions):\n",
    "        discounted_reward = 0\n",
    "        discounted_rewards = []\n",
    "        rewards.reverse()\n",
    "        for r in rewards:\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            discounted_rewards.append(discounted_reward)\n",
    "        discounted_rewards.reverse() \n",
    "        \n",
    "        for state, reward, action in zip(states, discounted_rewards, actions):\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probabilities = self.policy_net(np.array([state]), training=True)\n",
    "                loss = self.loss(action_probabilities, action, reward)\n",
    "            grads = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(grads, self.policy_net.trainable_variables)\n",
    "            )\n",
    "            \n",
    "    def loss(self, action_probabilities, action, reward):\n",
    "        dist = tfp.distributions.Categorical(\n",
    "            probs = action_probabilities, dtype=tf.float32\n",
    "        )\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob * reward\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4207bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, env: gym.Env, episodes: int, render=True):\n",
    "    \"\"\"\n",
    "    Train `agent` in `env` for `episodes`\n",
    "    \n",
    "    Args:\n",
    "        agent(Agent) : Agent to train\n",
    "        env (gym.Env): Environment to train the agent \n",
    "        episodes (int): Number of episodes to train\n",
    "        render (bool) : True=Enable/ False=Disable rendering\n",
    "    \"\"\"\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        done = False \n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = next_state\n",
    "            total_reward += reward \n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                agent.learn(states, rewards, actions)\n",
    "                print(\"\\n\")\n",
    "            print(f\"Episodes #{episode} ep_reward: {total_reward}\", end=\"\\r\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e46626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes #0 ep_reward: -199.0\n",
      "\n",
      "Episodes #1 ep_reward: -199.0\n",
      "\n",
      "Episodes #2 ep_reward: -199.0\n",
      "\n",
      "Episodes #3 ep_reward: -199.0\n",
      "\n",
      "Episodes #4 ep_reward: -199.0\n",
      "\n",
      "Episodes #5 ep_reward: -199.0\n",
      "\n",
      "Episodes #6 ep_reward: -199.0\n",
      "\n",
      "Episodes #7 ep_reward: -199.0\n",
      "\n",
      "Episodes #8 ep_reward: -199.0\n",
      "\n",
      "Episodes #9 ep_reward: -199.0\n",
      "\n",
      "Episodes #10 ep_reward: -199.0\n",
      "\n",
      "Episodes #11 ep_reward: -199.0\n",
      "\n",
      "Episodes #12 ep_reward: -199.0\n",
      "\n",
      "Episodes #13 ep_reward: -199.0\n",
      "\n",
      "Episodes #14 ep_reward: -199.0\n",
      "\n",
      "Episodes #15 ep_reward: -199.0\n",
      "\n",
      "Episodes #16 ep_reward: -199.0\n",
      "\n",
      "Episodes #17 ep_reward: -199.0\n",
      "\n",
      "Episodes #18 ep_reward: -199.0\n",
      "\n",
      "Episodes #19 ep_reward: -199.0\n",
      "\n",
      "Episodes #20 ep_reward: -199.0\n",
      "\n",
      "Episodes #21 ep_reward: -199.0\n",
      "\n",
      "Episodes #22 ep_reward: -199.0\n",
      "\n",
      "Episodes #23 ep_reward: -199.0\n",
      "\n",
      "Episodes #24 ep_reward: -199.0\n",
      "\n",
      "Episodes #25 ep_reward: -199.0\n",
      "\n",
      "Episodes #26 ep_reward: -199.0\n",
      "\n",
      "Episodes #27 ep_reward: -199.0\n",
      "\n",
      "Episodes #28 ep_reward: -199.0\n",
      "\n",
      "Episodes #29 ep_reward: -199.0\n",
      "\n",
      "Episodes #30 ep_reward: -199.0\n",
      "\n",
      "Episodes #31 ep_reward: -199.0\n",
      "\n",
      "Episodes #32 ep_reward: -199.0\n",
      "\n",
      "Episodes #33 ep_reward: -199.0\n",
      "\n",
      "Episodes #34 ep_reward: -199.0\n",
      "\n",
      "Episodes #35 ep_reward: -199.0\n",
      "\n",
      "Episodes #36 ep_reward: -199.0\n",
      "\n",
      "Episodes #37 ep_reward: -199.0\n",
      "\n",
      "Episodes #38 ep_reward: -199.0\n",
      "\n",
      "Episodes #39 ep_reward: -199.0\n",
      "\n",
      "Episodes #40 ep_reward: -199.0\n",
      "\n",
      "Episodes #41 ep_reward: -199.0\n",
      "\n",
      "Episodes #42 ep_reward: -199.0\n",
      "\n",
      "Episodes #43 ep_reward: -199.0\n",
      "\n",
      "Episodes #44 ep_reward: -199.0\n",
      "\n",
      "Episodes #45 ep_reward: -199.0\n",
      "\n",
      "Episodes #46 ep_reward: -199.0\n",
      "\n",
      "Episodes #47 ep_reward: -199.0\n",
      "\n",
      "Episodes #48 ep_reward: -199.0\n",
      "\n",
      "Episodes #49 ep_reward: -199.0\n",
      "\n",
      "Episodes #50 ep_reward: -199.0\n",
      "\n",
      "Episodes #51 ep_reward: -199.0\n",
      "\n",
      "Episodes #52 ep_reward: -199.0\n",
      "\n",
      "Episodes #53 ep_reward: -199.0\n",
      "\n",
      "Episodes #54 ep_reward: -199.0\n",
      "\n",
      "Episodes #55 ep_reward: -199.0\n",
      "\n",
      "Episodes #56 ep_reward: -199.0\n",
      "\n",
      "Episodes #57 ep_reward: -199.0\n",
      "\n",
      "Episodes #58 ep_reward: -199.0\n",
      "\n",
      "Episodes #59 ep_reward: -199.0\n",
      "\n",
      "Episodes #60 ep_reward: -199.0\n",
      "\n",
      "Episodes #61 ep_reward: -199.0\n",
      "\n",
      "Episodes #62 ep_reward: -199.0\n",
      "\n",
      "Episodes #63 ep_reward: -199.0\n",
      "\n",
      "Episodes #64 ep_reward: -199.0\n",
      "\n",
      "Episodes #65 ep_reward: -199.0\n",
      "\n",
      "Episodes #66 ep_reward: -199.0\n",
      "\n",
      "Episodes #67 ep_reward: -199.0\n",
      "\n",
      "Episodes #68 ep_reward: -199.0\n",
      "\n",
      "Episodes #69 ep_reward: -199.0\n",
      "\n",
      "Episodes #70 ep_reward: -199.0\n",
      "\n",
      "Episodes #71 ep_reward: -199.0\n",
      "\n",
      "Episodes #72 ep_reward: -199.0\n",
      "\n",
      "Episodes #73 ep_reward: -199.0\n",
      "\n",
      "Episodes #74 ep_reward: -199.0\n",
      "\n",
      "Episodes #75 ep_reward: -199.0\n",
      "\n",
      "Episodes #76 ep_reward: -199.0\n",
      "\n",
      "Episodes #77 ep_reward: -199.0\n",
      "\n",
      "Episodes #78 ep_reward: -199.0\n",
      "\n",
      "Episodes #79 ep_reward: -199.0\n",
      "\n",
      "Episodes #80 ep_reward: -199.0\n",
      "\n",
      "Episodes #81 ep_reward: -199.0\n",
      "\n",
      "Episodes #82 ep_reward: -199.0\n",
      "\n",
      "Episodes #83 ep_reward: -199.0\n",
      "\n",
      "Episodes #84 ep_reward: -199.0\n",
      "\n",
      "Episodes #85 ep_reward: -199.0\n",
      "\n",
      "Episodes #86 ep_reward: -199.0\n",
      "\n",
      "Episodes #87 ep_reward: -199.0\n",
      "\n",
      "Episodes #88 ep_reward: -199.0\n",
      "\n",
      "Episodes #89 ep_reward: -199.0\n",
      "\n",
      "Episodes #90 ep_reward: -199.0\n",
      "\n",
      "Episodes #91 ep_reward: -199.0\n",
      "\n",
      "Episodes #92 ep_reward: -199.0\n",
      "\n",
      "Episodes #93 ep_reward: -199.0\n",
      "\n",
      "Episodes #94 ep_reward: -199.0\n",
      "\n",
      "Episodes #95 ep_reward: -199.0\n",
      "\n",
      "Episodes #96 ep_reward: -199.0\n",
      "\n",
      "Episodes #97 ep_reward: -199.0\n",
      "\n",
      "Episodes #98 ep_reward: -199.0\n",
      "\n",
      "Episodes #99 ep_reward: -199.0\n",
      "\n",
      "Episodes #100 ep_reward: -199.0\n",
      "\n",
      "Episodes #101 ep_reward: -199.0\n",
      "\n",
      "Episodes #102 ep_reward: -199.0\n",
      "\n",
      "Episodes #103 ep_reward: -199.0\n",
      "\n",
      "Episodes #104 ep_reward: -199.0\n",
      "\n",
      "Episodes #105 ep_reward: -199.0\n",
      "\n",
      "Episodes #106 ep_reward: -199.0\n",
      "\n",
      "Episodes #107 ep_reward: -199.0\n",
      "\n",
      "Episodes #108 ep_reward: -199.0\n",
      "\n",
      "Episodes #109 ep_reward: -199.0\n",
      "\n",
      "Episodes #110 ep_reward: -199.0\n",
      "\n",
      "Episodes #111 ep_reward: -199.0\n",
      "\n",
      "Episodes #112 ep_reward: -199.0\n",
      "\n",
      "Episodes #113 ep_reward: -199.0\n",
      "\n",
      "Episodes #114 ep_reward: -199.0\n",
      "\n",
      "Episodes #115 ep_reward: -199.0\n",
      "\n",
      "Episodes #116 ep_reward: -199.0\n",
      "\n",
      "Episodes #117 ep_reward: -199.0\n",
      "\n",
      "Episodes #118 ep_reward: -199.0\n",
      "\n",
      "Episodes #119 ep_reward: -199.0\n",
      "\n",
      "Episodes #120 ep_reward: -199.0\n",
      "\n",
      "Episodes #121 ep_reward: -199.0\n",
      "\n",
      "Episodes #122 ep_reward: -199.0\n",
      "\n",
      "Episodes #123 ep_reward: -199.0\n",
      "\n",
      "Episodes #124 ep_reward: -199.0\n",
      "\n",
      "Episodes #125 ep_reward: -199.0\n",
      "\n",
      "Episodes #126 ep_reward: -199.0\n",
      "\n",
      "Episodes #127 ep_reward: -199.0\n",
      "\n",
      "Episodes #128 ep_reward: -199.0\n",
      "\n",
      "Episodes #129 ep_reward: -199.0\n",
      "\n",
      "Episodes #130 ep_reward: -199.0\n",
      "\n",
      "Episodes #131 ep_reward: -199.0\n",
      "\n",
      "Episodes #132 ep_reward: -199.0\n",
      "\n",
      "Episodes #133 ep_reward: -199.0\n",
      "\n",
      "Episodes #134 ep_reward: -199.0\n",
      "\n",
      "Episodes #135 ep_reward: -199.0\n",
      "\n",
      "Episodes #136 ep_reward: -199.0\n",
      "\n",
      "Episodes #137 ep_reward: -199.0\n",
      "\n",
      "Episodes #138 ep_reward: -199.0\n",
      "\n",
      "Episodes #139 ep_reward: -199.0\n",
      "\n",
      "Episodes #140 ep_reward: -199.0\n",
      "\n",
      "Episodes #141 ep_reward: -199.0\n",
      "\n",
      "Episodes #142 ep_reward: -199.0\n",
      "\n",
      "Episodes #143 ep_reward: -199.0\n",
      "\n",
      "Episodes #144 ep_reward: -199.0\n",
      "\n",
      "Episodes #145 ep_reward: -199.0\n",
      "\n",
      "Episodes #146 ep_reward: -199.0\n",
      "\n",
      "Episodes #147 ep_reward: -199.0\n",
      "\n",
      "Episodes #148 ep_reward: -199.0\n",
      "\n",
      "Episodes #149 ep_reward: -199.0\n",
      "\n",
      "Episodes #150 ep_reward: -199.0\n",
      "\n",
      "Episodes #151 ep_reward: -199.0\n",
      "\n",
      "Episodes #152 ep_reward: -199.0\n",
      "\n",
      "Episodes #153 ep_reward: -199.0\n",
      "\n",
      "Episodes #154 ep_reward: -199.0\n",
      "\n",
      "Episodes #155 ep_reward: -199.0\n",
      "\n",
      "Episodes #156 ep_reward: -199.0\n",
      "\n",
      "Episodes #157 ep_reward: -199.0\n",
      "\n",
      "Episodes #158 ep_reward: -199.0\n",
      "\n",
      "Episodes #159 ep_reward: -199.0\n",
      "\n",
      "Episodes #160 ep_reward: -199.0\n",
      "\n",
      "Episodes #161 ep_reward: -199.0\n",
      "\n",
      "Episodes #162 ep_reward: -199.0\n",
      "\n",
      "Episodes #163 ep_reward: -199.0\n",
      "\n",
      "Episodes #164 ep_reward: -199.0\n",
      "\n",
      "Episodes #165 ep_reward: -199.0\n",
      "\n",
      "Episodes #166 ep_reward: -199.0\n",
      "\n",
      "Episodes #167 ep_reward: -199.0\n",
      "\n",
      "Episodes #168 ep_reward: -199.0\n",
      "\n",
      "Episodes #169 ep_reward: -199.0\n",
      "\n",
      "Episodes #170 ep_reward: -199.0\n",
      "\n",
      "Episodes #171 ep_reward: -199.0\n",
      "\n",
      "Episodes #172 ep_reward: -199.0\n",
      "\n",
      "Episodes #173 ep_reward: -199.0\n",
      "\n",
      "Episodes #174 ep_reward: -199.0\n",
      "\n",
      "Episodes #175 ep_reward: -199.0\n",
      "\n",
      "Episodes #176 ep_reward: -199.0\n",
      "\n",
      "Episodes #177 ep_reward: -199.0\n",
      "\n",
      "Episodes #178 ep_reward: -199.0\n",
      "\n",
      "Episodes #179 ep_reward: -199.0\n",
      "\n",
      "Episodes #180 ep_reward: -199.0\n",
      "\n",
      "Episodes #181 ep_reward: -199.0\n",
      "\n",
      "Episodes #182 ep_reward: -199.0\n",
      "\n",
      "Episodes #183 ep_reward: -199.0\n",
      "\n",
      "Episodes #184 ep_reward: -199.0\n",
      "\n",
      "Episodes #185 ep_reward: -199.0\n",
      "\n",
      "Episodes #186 ep_reward: -199.0\n",
      "\n",
      "Episodes #187 ep_reward: -199.0\n",
      "\n",
      "Episodes #188 ep_reward: -199.0\n",
      "\n",
      "Episodes #189 ep_reward: -199.0\n",
      "\n",
      "Episodes #190 ep_reward: -199.0\n",
      "\n",
      "Episodes #191 ep_reward: -199.0\n",
      "\n",
      "Episodes #192 ep_reward: -199.0\n",
      "\n",
      "Episodes #193 ep_reward: -199.0\n",
      "\n",
      "Episodes #194 ep_reward: -199.0\n",
      "\n",
      "Episodes #195 ep_reward: -199.0\n",
      "\n",
      "Episodes #196 ep_reward: -199.0\n",
      "\n",
      "Episodes #197 ep_reward: -199.0\n",
      "\n",
      "Episodes #198 ep_reward: -199.0\n",
      "\n",
      "Episodes #199 ep_reward: -199.0\n",
      "\n",
      "Episodes #199 ep_reward: -200.0\r"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "episodes = 200\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "train(agent, env, episodes, render=False)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32f822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3812jvsc74a57bd0f14269bf3ad7aa2ae115b9ca9481e15e8eacecd83dc3347ac1efd388ad78cc6e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
